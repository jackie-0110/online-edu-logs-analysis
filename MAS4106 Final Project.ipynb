{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Struggles and Tutorial Impact in Online Education Logs\n",
    "## By: [John Courtney](mailto:jrcourtney3797@eagle.fgcu.edu) and [Alana D'Angelo](adangelo9184@eagle.fgcu.edu)\n",
    "#### MAS 4106 - Spring 2025\n",
    "#### Instructor: [Dr. Alberto A. Condori](mailto:acondori@fgcu.edu), [FGCU](http://www.fgcu.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract:\n",
    "The purpose of this material is to accurately make predictions on student performance to\n",
    "tailor their education to their ability. Riiid is the source company, an AI EdTech company\n",
    "based in South Korea. Enabling higher efficiency in education improves student\n",
    "outcomes, which in turn opens doors for social mobility. Meeting students with support\n",
    "for exactly where they're struggling is an efficient use of both the student and educators\n",
    "time. This is important to educators, students, parents, and nearly everyone with a stake\n",
    "in education.\n",
    "\n",
    "\n",
    "K-Means Clustering as well as a regularized least squares model was implemented. The\n",
    "k-means clustering revealed performance groups by capturing variation in accuracy,\n",
    "standard deviation, and number of attempts. The regularized least squares yielded with\n",
    "$\\lambda=0.01$ achieved ~80.7\\% accuracy and 100\\% precision in predicting student success. Feature engineering was required on the original data to include metrics\n",
    "such as mean accuracy, consistency trends, and effectiveness to train the model. These\n",
    "findings suggest that predictive modeling with performance-based clustering can support\n",
    "the development of personalized education strategies, since educators can reliably\n",
    "determine whether a student needs help or falls within a performance group that is classified as intervention needing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg; Pkg.add(\"LinearAlgebra\"); Pkg.add(\"Plots\"); Pkg.add(\"Statistics\"); Pkg.add(\"CSV\"); Pkg.add(\"DataFrames\"); Pkg.add(\"Random\"); Pkg.add(\"Clustering\"); Pkg.add(\"PlotlyJS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Plots\n",
    "using Statistics\n",
    "using CSV, DataFrames\n",
    "using Random\n",
    "\n",
    "df = DataFrame()\n",
    "\n",
    "for chunk in CSV.Chunks(\"train.csv\"; ntasks=10)\n",
    "    df_chunk = DataFrame(chunk)\n",
    "    df_chunk = filter(row -> rand() < 0.01, df_chunk) # basically just randomly takes 1% of the dataset for each chunk\n",
    "    filter!(row -> row.content_type_id == 0, df_chunk)\n",
    "    append!(df, df_chunk)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "grouped = combine(groupby(df[df.content_type_id .== 0, :], :user_id), \n",
    "  :answered_correctly => mean => :mean_acc,\n",
    "  :answered_correctly => (x -> isnan(std(x)) ? 0.0 : std(x)) => :std_acc,\n",
    "  :answered_correctly => length => :num_attempted,\n",
    "  :prior_question_elapsed_time => (x -> mean(skipmissing(x))) => :avg_time,\n",
    "  :prior_question_had_explanation => (x -> mean(skipmissing(x))) => :mean_explained \n",
    ")\n",
    "grouped = dropmissing(grouped)\n",
    "\n",
    "\n",
    "X = Matrix(grouped[:, [:mean_acc, :std_acc, :num_attempted, :avg_time, :mean_explained]])\n",
    "replace!(X, Inf=>0); replace!(X, NaN=>0)\n",
    "\n",
    "# Scale features\n",
    "function scale_features(X)\n",
    "    X_scaled = similar(X)\n",
    "    for i in 1:size(X, 2)\n",
    "        X_scaled[:, i] = (X[:, i] .- mean(X[:, i])) ./ std(X[:, i])\n",
    "    end\n",
    "    return X_scaled\n",
    "end\n",
    "\n",
    "#Regularization of the attempts as they can't be scaled with z-score.\n",
    "function regularize_attempts(X, λ=0.1)\n",
    "    X_reg = copy(X)\n",
    "    X_reg[:, 3] = X_reg[:, 3] * λ\n",
    "    return X_reg\n",
    "end\n",
    "\n",
    "\n",
    "function prepare_features(df)\n",
    "    \n",
    "    question_df = df[df.content_type_id .== 0, :]\n",
    "    sort!(question_df, [:user_id, :timestamp])\n",
    "    \n",
    "    \n",
    "    user_features = combine(groupby(question_df, :user_id)) do user_data\n",
    "        \n",
    "        mean_acc = mean(user_data.answered_correctly)\n",
    "        std_acc = isnan(std(user_data.answered_correctly)) ? 0.0 : std(user_data.answered_correctly)\n",
    "        \n",
    "        # Time features\n",
    "        time_diffs = diff(user_data.timestamp)\n",
    "        mean_time_between = mean(time_diffs)\n",
    "        std_time_between = isnan(std(time_diffs)) ? 0.0 : std(time_diffs)\n",
    "        \n",
    "        \n",
    "        recent_acc = mean(user_data.answered_correctly[max(1, end-4):end])\n",
    "        \n",
    "        # Performance trend\n",
    "        if length(user_data.answered_correctly) >= 10\n",
    "            first_half = mean(user_data.answered_correctly[1:div(end,2)])\n",
    "            second_half = mean(user_data.answered_correctly[div(end,2)+1:end])\n",
    "            trend = second_half - first_half\n",
    "        else\n",
    "            trend = 0.0\n",
    "        end\n",
    "        \n",
    "        \n",
    "        container_acc = combine(groupby(user_data, :task_container_id), \n",
    "            :answered_correctly => mean => :container_acc).container_acc\n",
    "        mean_container_acc = mean(container_acc)\n",
    "        std_container_acc = isnan(std(container_acc)) ? 0.0 : std(container_acc)\n",
    "        \n",
    "        \n",
    "        explanation_impact = 0.0\n",
    "        valid_explanations = 0\n",
    "        for i in 2:nrow(user_data)\n",
    "            if !ismissing(user_data.prior_question_had_explanation[i]) && user_data.prior_question_had_explanation[i]\n",
    "                explanation_impact += user_data.answered_correctly[i] - user_data.answered_correctly[i-1]\n",
    "                valid_explanations += 1\n",
    "            end\n",
    "        end\n",
    "        explanation_impact = valid_explanations > 0 ? explanation_impact / valid_explanations : 0.0\n",
    "        \n",
    "        # Time-accuracy\n",
    "        time_values = collect(skipmissing(user_data.prior_question_elapsed_time))\n",
    "        acc_values = float.(user_data.answered_correctly[.!ismissing.(user_data.prior_question_elapsed_time)])\n",
    "        \n",
    "        if length(time_values) > 1 && length(acc_values) > 1\n",
    "            time_acc_correlation = cor(time_values, acc_values)\n",
    "            time_acc_correlation = isnan(time_acc_correlation) ? 0.0 : time_acc_correlation\n",
    "        else\n",
    "            time_acc_correlation = 0.0\n",
    "        end\n",
    "        \n",
    "        return (\n",
    "            mean_acc = mean_acc,\n",
    "            std_acc = std_acc,\n",
    "            num_questions = nrow(user_data),\n",
    "            mean_time_between = mean_time_between,\n",
    "            std_time_between = std_time_between,\n",
    "            recent_acc = recent_acc,\n",
    "            performance_trend = trend,\n",
    "            mean_container_acc = mean_container_acc,\n",
    "            std_container_acc = std_container_acc,\n",
    "            explanation_impact = Float64(explanation_impact),\n",
    "            time_acc_correlation = time_acc_correlation\n",
    "        )\n",
    "    end\n",
    "    \n",
    "    # Feature Matrix\n",
    "    feature_cols = [:mean_acc, :std_acc, :num_questions, :mean_time_between, \n",
    "                   :std_time_between, :recent_acc, :performance_trend, \n",
    "                   :mean_container_acc, :std_container_acc, :explanation_impact, \n",
    "                   :time_acc_correlation]\n",
    "    \n",
    "    for col in feature_cols\n",
    "        user_features[!, col] = float.(user_features[!, col])\n",
    "    end\n",
    "    \n",
    "    X = Matrix{Float64}(user_features[:, feature_cols])\n",
    "    replace!(X, NaN=>0.0, Inf=>0.0, -Inf=>0.0)\n",
    "    \n",
    "    # Normalization\n",
    "    X = (X .- mean(X, dims=1)) ./ std(X, dims=1)\n",
    "    \n",
    "    # Target value\n",
    "    y = float.(user_features.mean_acc .>= median(user_features.mean_acc))\n",
    "    \n",
    "    return X, y, user_features, feature_cols\n",
    "end\n",
    "\n",
    "# Just the regularized least squares model implemented.\n",
    "function regularized_ls(X, y, λ)\n",
    "    n, p = size(X)\n",
    "    β = (X'X + λ*I(p)) \\ (X'y)\n",
    "    return β\n",
    "end\n",
    "\n",
    "# Predicts the value of y given X and β.\n",
    "function predict(X, β)\n",
    "    return X * β\n",
    "end\n",
    "\n",
    "# Evaluates the accuracy of the model by comparing the predicted values to the true values.\n",
    "function evaluate_model(y_true, y_pred)\n",
    "    predictions = y_pred .>= 0.5\n",
    "    accuracy = mean(predictions .== y_true)\n",
    "    return accuracy\n",
    "end\n",
    "\n",
    "# Splits the data into training and testing sets.\n",
    "function split_data(X, y, train_ratio=0.8)\n",
    "    n = size(X, 1)\n",
    "    shuffle_idx = randperm(n)\n",
    "    train_size = floor(Int, train_ratio * n)\n",
    "    \n",
    "    X_train = X[shuffle_idx[1:train_size], :]\n",
    "    y_train = y[shuffle_idx[1:train_size]]\n",
    "    X_test = X[shuffle_idx[(train_size+1):end], :]\n",
    "    y_test = y[shuffle_idx[(train_size+1):end]]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "end\n",
    "\n",
    "# Confusion matrix\n",
    "function print_confusion_matrix_table(X, y, β)\n",
    "    y_pred_continuous = predict(X, β)\n",
    "    y_pred_binary = y_pred_continuous .>= 0.5\n",
    "    \n",
    "    # Metrics\n",
    "    tp = sum((y .== 1) .& (y_pred_binary .== 1))\n",
    "    fp = sum((y .== 0) .& (y_pred_binary .== 1))\n",
    "    tn = sum((y .== 0) .& (y_pred_binary .== 0))\n",
    "    fn = sum((y .== 1) .& (y_pred_binary .== 0))\n",
    "    \n",
    "    total = tp + fp + tn + fn\n",
    "    p_total = sum(y .== 1)\n",
    "    n_total = sum(y .== 0)\n",
    "    \n",
    "    tp_pct = round(tp / total * 100, digits=2)\n",
    "    fp_pct = round(fp / total * 100, digits=2)\n",
    "    tn_pct = round(tn / total * 100, digits=2)\n",
    "    fn_pct = round(fn / total * 100, digits=2)\n",
    "    \n",
    "    accuracy = (tp + tn) / total\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    println(\"Confusion Matrix:\")\n",
    "    println(\"=================\")\n",
    "    println(\"                 Predicted Negative    Predicted Positive    Total\")\n",
    "    println(\"Actual Negative  $tn ($tn_pct%)        $fp ($fp_pct%)          $n_total\")\n",
    "    println(\"Actual Positive  $fn ($fn_pct%)        $tp ($tp_pct%)          $p_total\")\n",
    "    println(\"Total            $(tn + fn)            $(fp + tp)              $total\")\n",
    "    println()\n",
    "    println(\"Metrics:\")\n",
    "    println(\"=========\")\n",
    "    println(\"Accuracy:  $(round(accuracy * 100, digits=2))%\")\n",
    "    println(\"Precision: $(round(precision * 100, digits=2))%\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Clustering\n",
    "K = 20\n",
    "results = kmeans(X', K; maxiter=20)\n",
    "assignments = results.assignments\n",
    "\n",
    "gr()\n",
    "\n",
    "X_3d = X[:, [1, 2, 3]]\n",
    "\n",
    "p2 = Plots.scatter(X_3d[:, 1], X_3d[:, 2], X_3d[:, 3],\n",
    "    zcolor = assignments,\n",
    "    markersize = 3,\n",
    "    xlabel = \"Mean Accuracy\",\n",
    "    ylabel = \"Std Dev Accuracy\",\n",
    "    zlabel = \"Number of Attempts\",\n",
    "    title = \"Student Clusters: Performance vs Engagement\",\n",
    "    colorbar_title = \"Cluster\",\n",
    "    legend = false\n",
    ")\n",
    "\n",
    "display(p2)\n",
    "\n",
    "\n",
    "\n",
    "# Elbow method\n",
    "m, n = size(X)\n",
    "J = zeros(K)\n",
    "for k in 1:K\n",
    "    result = kmeans(X', k; maxiter=100)\n",
    "    assignments = result.assignments\n",
    "    centers = result.centers\n",
    "    J[k] = sum(norm(X[i,:] - centers[:, assignments[i]], 2)^2 for i in 1:m)\n",
    "end\n",
    "\n",
    "p3 = Plots.plot(1:K, J, marker=:circle, \n",
    "    xlabel=\"k\", ylabel=\"SSE\", \n",
    "    title=\"SSE vs. k (Elbow Method)\", \n",
    "    legend=false)\n",
    "display(p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, user_features, feature_cols = prepare_features(df)\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_data(X, y)\n",
    "\n",
    "β = regularized_ls(X_train, y_train, .01)\n",
    "\n",
    "metrics = print_confusion_matrix_table(X_test, y_test, β)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
